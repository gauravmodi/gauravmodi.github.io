{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Title: Cleaning Text Data (Tokenization)\n",
    "+ Author: Gaurav Modi\n",
    "+ Date: 2018-05-01\n",
    "+ Description: \"Cleaning Text Data using NLTK\"\n",
    "+ Slug: cleaning_text_data\n",
    "+ Tags: nltk, cleaning, tokenization, word tokenization, sentence tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to clean data**\n",
    "- Read data file\n",
    "- Convert text into word list\n",
    "- Remove punctuations\n",
    "- Sentence Tokenization\n",
    "- Word Tokenization\n",
    "- Removing Stop Words\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2922,
     "status": "ok",
     "timestamp": 1526262673885,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "qjx0Nc2lAZWa",
    "outputId": "80bbecf3-40c7-4817-ae09-ba8170695b12"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: http://www.gutenberg.org/ebooks/1661 *(Plain Text UTF-8 version)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1526262674980,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "uP9zgh3CBlnZ",
    "outputId": "796c8459-eb75-4a3a-9916-ebdbdb6b755e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', \"Gutenberg's\", 'The', 'Adventures', 'of', 'Sherlock', 'Holmes,', 'by', 'Arthur', 'Conan']\n"
     ]
    }
   ],
   "source": [
    "file_path = pathlib.Path('./data/sherlock_holmes/the_adventures_of_sherlock_holmes.txt')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()       \n",
    "    f.close()\n",
    "    \n",
    "words = text[1:].split()\n",
    "\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYMfY163UyzM"
   },
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1526262676583,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "iO9yu8RyUxvm",
    "outputId": "77114cc6-c5e1-49c5-8485-a53048749cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenbergs', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan']\n"
     ]
    }
   ],
   "source": [
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "words = [word.translate(punctuation_table).lower() for word in words]\n",
    "\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azJmxnvXRrlI"
   },
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1526262678779,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "zrQmKDvfJUDV",
    "outputId": "a35fadce-4363-4232-d524-d7df18749a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is peculiarly\n",
      "strong and stiff.\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQLtLb1sRwWu"
   },
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1526262681728,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "8TinkBiKQSNv",
    "outputId": "1a056a93-49b6-4445-f649-d41a0c4c4913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 105766\n",
      "['project', 'gutenberg', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text[1:])\n",
    "words = [token.lower() for token in tokens if token.isalpha()]\n",
    "print(f'Number of words: {len(words)}')\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJjxrxUfSZZz"
   },
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1526262683767,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "cs6UqZGCQwpD",
    "outputId": "e15a02e0-ee43-4a84-8a68-7ae674c3cc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after removing Stop Words: 46660\n",
      "['project', 'gutenberg', 'adventures', 'sherlock', 'holmes', 'arthur', 'conan', 'doyle', 'ebook', 'use']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(f'Number of words after removing Stop Words: {len(words)}')\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4ecMj39RcRv"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1448,
     "status": "ok",
     "timestamp": 1526262687692,
     "user": {
      "displayName": "Gaurav Modi",
      "photoUrl": "//lh6.googleusercontent.com/-3oJDWhTfBiM/AAAAAAAAAAI/AAAAAAAAACk/3QqJRYQTYsA/s50-c-k-no/photo.jpg",
      "userId": "104261121391640075616"
     },
     "user_tz": 300
    },
    "id": "zgPadTneTLXp",
    "outputId": "2c5507f0-eae2-4328-e8e4-a1e273c930b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'adventur', 'sherlock', 'holm', 'arthur', 'conan', 'doyl', 'ebook', 'use']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lK0PGknfTUXi"
   },
   "source": [
    "### Additional possible cleaning Steps for other data\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vETs9rYbTl2H"
   },
   "source": [
    "### Tomas Mikolov (one of the developers of word2vec) on Text Cleaning for Word Embeddings\n",
    "\n",
    "*There is no universal answer. It all depends on what you plan to use the vectors for. In my experience, it is usually good to disconnect (or remove) punctuation from words, and sometimes also convert all characters to lowercase. One can also replace all numbers (possibly greater than some constant) with some single token such as .*\n",
    "\n",
    "*All these pre-processing steps aim to reduce the vocabulary size without removing any important content (which in some cases may not be true when you lowercase certain words, ie. ‘Bush’ is different than ‘bush’, while ‘Another’ has usually the same sense as ‘another’). The smaller the vocabulary is, the lower is the memory complexity, and the more robustly are the parameters for the words estimated. You also have to pre-process the test data in the same way.*\n",
    "\n",
    "…\n",
    "\n",
    "*In short, you will understand all this much better if you will run experiments.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "NLP 1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
